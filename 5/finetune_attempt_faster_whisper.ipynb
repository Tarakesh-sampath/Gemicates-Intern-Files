{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ff6aae949ba14e54b3317f42dccc1811": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e02d69379c924b628b79936680bfced6",
              "IPY_MODEL_78c9cfde02f64730bbf7641f8ceaa96a",
              "IPY_MODEL_700f75957c91469c8d7fc61b1ef579dd",
              "IPY_MODEL_dc6e12dc9cdb4badba1878bf55158b2c",
              "IPY_MODEL_f94b1890c98b4529898a136e2d3f62a7"
            ],
            "layout": "IPY_MODEL_41febc0189994a31aa69f5530328c9b8"
          }
        },
        "e02d69379c924b628b79936680bfced6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb2a5e38f907477a9ce30f51a20e9cf3",
            "placeholder": "​",
            "style": "IPY_MODEL_37925d3343c743029c9ce2bc212d0c68",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "78c9cfde02f64730bbf7641f8ceaa96a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_6ad382ff55824bef8d90b47edd05db3d",
            "placeholder": "​",
            "style": "IPY_MODEL_de5eee2a6d38497c96a9376613b5e35e",
            "value": ""
          }
        },
        "700f75957c91469c8d7fc61b1ef579dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_30e5f25460774980888d531b4a6cc9e5",
            "style": "IPY_MODEL_785996ab02f441f09e2a4cef5134b19c",
            "value": true
          }
        },
        "dc6e12dc9cdb4badba1878bf55158b2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_f9440c14a21c4966aef8c9f800e061a3",
            "style": "IPY_MODEL_31b3682af8c74657b47219fffd77601c",
            "tooltip": ""
          }
        },
        "f94b1890c98b4529898a136e2d3f62a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ad4e85354ef43d9a19c38183308fdcd",
            "placeholder": "​",
            "style": "IPY_MODEL_e32b6ece47d549778b3aab98b5f36b73",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "41febc0189994a31aa69f5530328c9b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "eb2a5e38f907477a9ce30f51a20e9cf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37925d3343c743029c9ce2bc212d0c68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ad382ff55824bef8d90b47edd05db3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de5eee2a6d38497c96a9376613b5e35e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30e5f25460774980888d531b4a6cc9e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "785996ab02f441f09e2a4cef5134b19c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9440c14a21c4966aef8c9f800e061a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31b3682af8c74657b47219fffd77601c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "7ad4e85354ef43d9a19c38183308fdcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e32b6ece47d549778b3aab98b5f36b73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94d7e2141ad04657ac8ff17e90033471": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7acbbf21bbbf45e290d84c89e01d58f7",
              "IPY_MODEL_7d4b6bd24f1c4af9b4cefdea7c5168da",
              "IPY_MODEL_14ad74fa7e7b4c7f9b8796989f3f4428"
            ],
            "layout": "IPY_MODEL_3ef9c9bf5e974af894dabfb112f978c0"
          }
        },
        "7acbbf21bbbf45e290d84c89e01d58f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6164883e86441e0ac56c7ae67666f4e",
            "placeholder": "​",
            "style": "IPY_MODEL_ed403a79c3f04ec0b25ce9eda0ea2de4",
            "value": "adapter_model.safetensors: 100%"
          }
        },
        "7d4b6bd24f1c4af9b4cefdea7c5168da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85248af144154555aae7885a48ff6213",
            "max": 14176064,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_418191634abf4083b4dd512bab1f6deb",
            "value": 14176064
          }
        },
        "14ad74fa7e7b4c7f9b8796989f3f4428": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0d001a25e49458cbf4d6dd9d7d42086",
            "placeholder": "​",
            "style": "IPY_MODEL_d5aa7e16efb0493ca9fb6e52c836b147",
            "value": " 14.2M/14.2M [00:00&lt;00:00, 24.7MB/s]"
          }
        },
        "3ef9c9bf5e974af894dabfb112f978c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6164883e86441e0ac56c7ae67666f4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed403a79c3f04ec0b25ce9eda0ea2de4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85248af144154555aae7885a48ff6213": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "418191634abf4083b4dd512bab1f6deb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0d001a25e49458cbf4d6dd9d7d42086": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5aa7e16efb0493ca9fb6e52c836b147": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkZEjIn4-hUO",
        "outputId": "ce334434-921e-4283-8f4d-dba1d8c32ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kylvWRB5tInq",
        "outputId": "876c7a8e-5485-486d-e76b-d99afb632448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets librosa evaluate jiwer gradio bitsandbytes accelerate\n",
        "!pip install -q git+https://github.com/huggingface/peft.git@main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi141HUytZgc",
        "outputId": "81c07bb1-810b-4e53-8ef4-76dba2ae0c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec  2 06:19:30 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "wYZc6dUP0iVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415,
          "referenced_widgets": [
            "ff6aae949ba14e54b3317f42dccc1811",
            "e02d69379c924b628b79936680bfced6",
            "78c9cfde02f64730bbf7641f8ceaa96a",
            "700f75957c91469c8d7fc61b1ef579dd",
            "dc6e12dc9cdb4badba1878bf55158b2c",
            "f94b1890c98b4529898a136e2d3f62a7",
            "41febc0189994a31aa69f5530328c9b8",
            "eb2a5e38f907477a9ce30f51a20e9cf3",
            "37925d3343c743029c9ce2bc212d0c68",
            "6ad382ff55824bef8d90b47edd05db3d",
            "de5eee2a6d38497c96a9376613b5e35e",
            "30e5f25460774980888d531b4a6cc9e5",
            "785996ab02f441f09e2a4cef5134b19c",
            "f9440c14a21c4966aef8c9f800e061a3",
            "31b3682af8c74657b47219fffd77601c",
            "7ad4e85354ef43d9a19c38183308fdcd",
            "e32b6ece47d549778b3aab98b5f36b73"
          ]
        },
        "id": "7GtljeKU0k5X",
        "outputId": "554b5c8a-be50-4e2e-ec52-74d4c3e81b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff6aae949ba14e54b3317f42dccc1811"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"openai/whisper-small.en\"\n",
        "task = \"transcribe\""
      ],
      "metadata": {
        "id": "U3eEqBr50oiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"Tarakeshwaran/Whisper-train-data\"\n",
        "language = \"English\"\n",
        "language_abbr = \"en\" # Short hand code for the language we want to fine-tune"
      ],
      "metadata": {
        "id": "H960ZRLX0r17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "whisper_data = DatasetDict()\n",
        "\n",
        "whisper_data[\"train\"] = load_dataset(dataset_name, split=\"train\")\n",
        "whisper_data[\"test\"] = load_dataset(dataset_name, split=\"test\")\n",
        "\n",
        "print(whisper_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48TVjcLO0tSl",
        "outputId": "a9d1a1ad-dde0-4533-bfd5-2c69df47d5d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['audio', 'text', 'start', 'end'],\n",
            "        num_rows: 80\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['audio', 'text', 'start', 'end'],\n",
            "        num_rows: 20\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "whisper_data = whisper_data.remove_columns(\n",
        "    [\"start\",\"end\"]\n",
        ")\n",
        "\n",
        "print(whisper_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kae15a1I0vix",
        "outputId": "316fc961-c576-4a83-b45a-1915870f8974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['audio', 'text'],\n",
            "        num_rows: 80\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['audio', 'text'],\n",
            "        num_rows: 20\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)"
      ],
      "metadata": {
        "id": "LMAeElFF0yZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)"
      ],
      "metadata": {
        "id": "0iqM0zPW01Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
      ],
      "metadata": {
        "id": "CjmEbD_b0151"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(whisper_data[\"train\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co8q6Ibx08RM",
        "outputId": "2cb184bc-eb63-46a4-c04e-88ced25a10aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'audio': {'path': 'sample-005811.mp3', 'array': array([-5.06648125e-24,  0.00000000e+00, -3.61891518e-24, ...,\n",
            "       -1.07697160e-05,  2.15555119e-05, -2.88033334e-05]), 'sampling_rate': 16000}, 'text': \"in alchemy it's called the soul of the world\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Audio\n",
        "\n",
        "whisper_data = whisper_data.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ],
      "metadata": {
        "id": "dwas8pcZ09wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(whisper_data[\"train\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdAoLD4E1A3m",
        "outputId": "4d79077c-044c-4638-f997-10fea1f60c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'audio': {'path': 'sample-005811.mp3', 'array': array([-5.06648125e-24,  0.00000000e+00, -3.61891518e-24, ...,\n",
            "       -1.07697160e-05,  2.15555119e-05, -2.88033334e-05]), 'sampling_rate': 16000}, 'text': \"in alchemy it's called the soul of the world\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"text\"]).input_ids\n",
        "    return batch"
      ],
      "metadata": {
        "id": "3ORvWe0X1F85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whisper_data = whisper_data.map(prepare_dataset, remove_columns=whisper_data.column_names[\"train\"], num_proc=2)"
      ],
      "metadata": {
        "id": "JVuOOEgm1Jsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whisper_data[\"train\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWuCjDuA1MUe",
        "outputId": "37fdd669-b434-4757-b833-e2791f2cbe0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_features', 'labels'],\n",
              "    num_rows: 80\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "QfHD7qiG1f5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ],
      "metadata": {
        "id": "mccANpEy1jqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ],
      "metadata": {
        "id": "w13ZpjI21l_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ],
      "metadata": {
        "id": "huIz0g9EAHru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, load_in_8bit=True, device_map=\"auto\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKbiAuYx1o2X",
        "outputId": "45fbf731-6e7f-44e6-c565-380f29866373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "# Assume `model` is your pre-trained transformer model\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "8EJUW3S69Kew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_inputs_require_grad(module, input, output):\n",
        "    output.requires_grad_(True)\n",
        "\n",
        "model.model.encoder.conv1.register_forward_hook(make_inputs_require_grad)"
      ],
      "metadata": {
        "id": "Ijud8U8-15nt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c4ecdd-c4a7-4a95-9a7b-0e82658c5d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.hooks.RemovableHandle at 0x7a3a13270cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qBHSiPc9QCP",
        "outputId": "f6c839e6-6f0a-4680-ae27-61c066acdb53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 3,538,944 || all params: 245,273,856 || trainable%: 1.4429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./faster-whisper-small-en\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-3,\n",
        "    warmup_steps=50,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    fp16=True,\n",
        "    save_steps=10,\n",
        "    eval_steps=10,\n",
        "    per_device_eval_batch_size=8,\n",
        "    generation_max_length=128,\n",
        "    predict_with_generate=True,\n",
        "    logging_steps=25,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    max_steps=500, # only for testing purposes, remove this from your final run :)\n",
        "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
        "    label_names=[\"labels\"],  # same reason as above\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8BHlr_E9ScP",
        "outputId": "8013273e-5871-4fe1-a286-fbcb58061acb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
        "\n",
        "# This callback helps to save only the adapter weights and remove the base model weights.\n",
        "class SavePeftModelCallback(TrainerCallback):\n",
        "    def on_save(\n",
        "        self,\n",
        "        args: TrainingArguments,\n",
        "        state: TrainerState,\n",
        "        control: TrainerControl,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
        "\n",
        "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
        "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
        "\n",
        "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
        "        if os.path.exists(pytorch_model_path):\n",
        "            os.remove(pytorch_model_path)\n",
        "        return control\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=whisper_data[\"train\"],\n",
        "    eval_dataset=whisper_data[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    callbacks=[SavePeftModelCallback],\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0z_4b229YAe",
        "outputId": "72917af3-1dbc-4875-c4ac-c81cb7b0f174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-ef3d940c9cc0>:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ouyNYWo29aw1",
        "outputId": "54594465-3be9-482d-fbe1-98d3c4eace1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtarakeshsampath1\u001b[0m (\u001b[33mtarakeshsampath1-none\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241202_062008-3oj4fz8m</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tarakeshsampath1-none/huggingface/runs/3oj4fz8m' target=\"_blank\">./faster-whisper-small-en</a></strong> to <a href='https://wandb.ai/tarakeshsampath1-none/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/tarakeshsampath1-none/huggingface' target=\"_blank\">https://wandb.ai/tarakeshsampath1-none/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/tarakeshsampath1-none/huggingface/runs/3oj4fz8m' target=\"_blank\">https://wandb.ai/tarakeshsampath1-none/huggingface/runs/3oj4fz8m</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 50:55, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.769154</td>\n",
              "      <td>25.853659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.278829</td>\n",
              "      <td>155.609756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.421900</td>\n",
              "      <td>0.734945</td>\n",
              "      <td>4.878049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.421900</td>\n",
              "      <td>0.513023</td>\n",
              "      <td>4.878049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.474800</td>\n",
              "      <td>0.102858</td>\n",
              "      <td>2.926829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.474800</td>\n",
              "      <td>0.083441</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.474800</td>\n",
              "      <td>0.078193</td>\n",
              "      <td>3.902439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.008500</td>\n",
              "      <td>0.074802</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.008500</td>\n",
              "      <td>0.075127</td>\n",
              "      <td>3.414634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>0.084832</td>\n",
              "      <td>3.414634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>0.089780</td>\n",
              "      <td>3.414634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>5.309700</td>\n",
              "      <td>65.853659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.593100</td>\n",
              "      <td>4.217952</td>\n",
              "      <td>83.414634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.593100</td>\n",
              "      <td>3.765716</td>\n",
              "      <td>136.585366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>4.175400</td>\n",
              "      <td>2.933682</td>\n",
              "      <td>81.463415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>4.175400</td>\n",
              "      <td>2.627402</td>\n",
              "      <td>170.731707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>4.175400</td>\n",
              "      <td>2.185328</td>\n",
              "      <td>90.731707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>2.366100</td>\n",
              "      <td>0.805332</td>\n",
              "      <td>20.975610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>2.366100</td>\n",
              "      <td>0.151111</td>\n",
              "      <td>6.829268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.261500</td>\n",
              "      <td>0.109534</td>\n",
              "      <td>3.902439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.261500</td>\n",
              "      <td>0.170264</td>\n",
              "      <td>11.219512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.261500</td>\n",
              "      <td>0.137440</td>\n",
              "      <td>6.829268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.027000</td>\n",
              "      <td>0.121432</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.027000</td>\n",
              "      <td>0.117668</td>\n",
              "      <td>7.804878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.002100</td>\n",
              "      <td>0.118685</td>\n",
              "      <td>7.804878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.002100</td>\n",
              "      <td>0.113398</td>\n",
              "      <td>7.317073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.002100</td>\n",
              "      <td>0.111296</td>\n",
              "      <td>64.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.110881</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.111656</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.112404</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.113923</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.111773</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.114614</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.116455</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.113577</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.116753</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.115746</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.115721</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.116134</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.115165</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.114396</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.116157</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.113260</td>\n",
              "      <td>4.878049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.111305</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.114483</td>\n",
              "      <td>4.878049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.114068</td>\n",
              "      <td>5.853659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.116562</td>\n",
              "      <td>5.853659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.118355</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.116667</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.114172</td>\n",
              "      <td>4.390244</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=500, training_loss=0.5669511904753745, metrics={'train_runtime': 3064.3353, 'train_samples_per_second': 1.305, 'train_steps_per_second': 0.163, 'total_flos': 1.17472591872e+18, 'train_loss': 0.5669511904753745, 'epoch': 50.0})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_id = \"Tarakeshwaran/faster-whisper-small-en\"\n",
        "model.push_to_hub(peft_model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159,
          "referenced_widgets": [
            "94d7e2141ad04657ac8ff17e90033471",
            "7acbbf21bbbf45e290d84c89e01d58f7",
            "7d4b6bd24f1c4af9b4cefdea7c5168da",
            "14ad74fa7e7b4c7f9b8796989f3f4428",
            "3ef9c9bf5e974af894dabfb112f978c0",
            "c6164883e86441e0ac56c7ae67666f4e",
            "ed403a79c3f04ec0b25ce9eda0ea2de4",
            "85248af144154555aae7885a48ff6213",
            "418191634abf4083b4dd512bab1f6deb",
            "e0d001a25e49458cbf4d6dd9d7d42086",
            "d5aa7e16efb0493ca9fb6e52c836b147"
          ]
        },
        "id": "ukNCyLNk9cGw",
        "outputId": "0e7c77d6-b614-4c60-c8f4-64e2e907e9ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/14.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94d7e2141ad04657ac8ff17e90033471"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/Tarakeshwaran/faster-whisper-small-en/commit/7fb1694df51792a0b8c6aed0a2aac1c54a1e362e', commit_message='Upload model', commit_description='', oid='7fb1694df51792a0b8c6aed0a2aac1c54a1e362e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Tarakeshwaran/faster-whisper-small-en', endpoint='https://huggingface.co', repo_type='model', repo_id='Tarakeshwaran/faster-whisper-small-en'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "!zip -r /content/faster-whisper-small-en.zip /content/faster-whisper-small-en\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZiQbDDOYEcUB",
        "outputId": "0e52e8da-2e44-4e54-f715-cd72f27bda1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/faster-whisper-small-en/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-440/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-440/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-440/trainer_state.json (deflated 82%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-440/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-440/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-440/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-440/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-440/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-440/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-440/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-440/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-440/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-440/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-440/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-260/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-260/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-260/trainer_state.json (deflated 79%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-260/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-260/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-260/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-260/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-260/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-260/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-260/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-260/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-260/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-260/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-260/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-70/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-70/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-70/trainer_state.json (deflated 71%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-70/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-70/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-70/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-70/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-70/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-70/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-70/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-70/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-70/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-70/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-70/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-100/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-100/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-100/trainer_state.json (deflated 74%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-100/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-100/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-100/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-100/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-100/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-100/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-100/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-100/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-100/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-100/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-100/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-30/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-30/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-30/trainer_state.json (deflated 63%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-30/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-30/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-30/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-30/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-30/adapter_model/adapter_model.safetensors (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-30/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-30/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-30/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-30/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-30/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-30/adapter_model.safetensors (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-290/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-290/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-290/trainer_state.json (deflated 80%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-290/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-290/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-290/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-290/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-290/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-290/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-290/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-290/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-290/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-290/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-290/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-160/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-160/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-160/trainer_state.json (deflated 77%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-160/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-160/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-160/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-160/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-160/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-160/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-160/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-160/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-160/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-160/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-160/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-90/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-90/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-90/trainer_state.json (deflated 73%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-90/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-90/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-90/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-90/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-90/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-90/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-90/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-90/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-90/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-90/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-90/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-500/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-500/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-500/trainer_state.json (deflated 83%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-500/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-500/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-500/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-500/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-500/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-500/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-500/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-500/scheduler.pt (deflated 55%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-500/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-500/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-500/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-280/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-280/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-280/trainer_state.json (deflated 80%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-280/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-280/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-280/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-280/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-280/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-280/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-280/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-280/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-280/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-280/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-280/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-340/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-340/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-340/trainer_state.json (deflated 81%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-340/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-340/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-340/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-340/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-340/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-340/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-340/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-340/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-340/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-340/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-340/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-320/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-320/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-320/trainer_state.json (deflated 80%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-320/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-320/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-320/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-320/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-320/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-320/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-320/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-320/scheduler.pt (deflated 55%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-320/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-320/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-320/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-480/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-480/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-480/trainer_state.json (deflated 83%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-480/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-480/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-480/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-480/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-480/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-480/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-480/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-480/scheduler.pt (deflated 55%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-480/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-480/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-480/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-370/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-370/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-370/trainer_state.json (deflated 81%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-370/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-370/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-370/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-370/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-370/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-370/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-370/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-370/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-370/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-370/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-370/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-380/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-380/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-380/trainer_state.json (deflated 81%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-380/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-380/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-380/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-380/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-380/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-380/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-380/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-380/scheduler.pt (deflated 55%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-380/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-380/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-380/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-50/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-50/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-50/trainer_state.json (deflated 68%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-50/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-50/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-50/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-50/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-50/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-50/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-50/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-50/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-50/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-50/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-50/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-350/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-350/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-350/trainer_state.json (deflated 81%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-350/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-350/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-350/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-350/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-350/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-350/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-350/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-350/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-350/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-350/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-350/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-170/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-170/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-170/trainer_state.json (deflated 77%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-170/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-170/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-170/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-170/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-170/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-170/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-170/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-170/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-170/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-170/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-170/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-140/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-140/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-140/trainer_state.json (deflated 76%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-140/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-140/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-140/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-140/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-140/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-140/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-140/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-140/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-140/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-140/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-140/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-310/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-310/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-310/trainer_state.json (deflated 80%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-310/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-310/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-310/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-310/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-310/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-310/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-310/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-310/scheduler.pt (deflated 55%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-310/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-310/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-310/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-210/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-210/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-210/trainer_state.json (deflated 78%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-210/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-210/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-210/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-210/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-210/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-210/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-210/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-210/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-210/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-210/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-210/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-240/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-240/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-240/trainer_state.json (deflated 79%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-240/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-240/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-240/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-240/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-240/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-240/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-240/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-240/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-240/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-240/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-240/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-420/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-420/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-420/trainer_state.json (deflated 82%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-420/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-420/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-420/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-420/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-420/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-420/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-420/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-420/scheduler.pt (deflated 55%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-420/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-420/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-420/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-80/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-80/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-80/trainer_state.json (deflated 72%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-80/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-80/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-80/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-80/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-80/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-80/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-80/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-80/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-80/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-80/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-80/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-390/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-390/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-390/trainer_state.json (deflated 82%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-390/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-390/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-390/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-390/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-390/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-390/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-390/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-390/scheduler.pt (deflated 55%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-390/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-390/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-390/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-450/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-450/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-450/trainer_state.json (deflated 82%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-450/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-450/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-450/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-450/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-450/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-450/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-450/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-450/scheduler.pt (deflated 55%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-450/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-450/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-450/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-60/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-60/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-60/trainer_state.json (deflated 70%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-60/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-60/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-60/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-60/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-60/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-60/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-60/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-60/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-60/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-60/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-60/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-230/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-230/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-230/trainer_state.json (deflated 79%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-230/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-230/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-230/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-230/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-230/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-230/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-230/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-230/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-230/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-230/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-230/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-250/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-250/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-250/trainer_state.json (deflated 79%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-250/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-250/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-250/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-250/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-250/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-250/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-250/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-250/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-250/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-250/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-250/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-360/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-360/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-360/trainer_state.json (deflated 81%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-360/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-360/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-360/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-360/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-360/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-360/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-360/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-360/scheduler.pt (deflated 55%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-360/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-360/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-360/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-110/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-110/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-110/trainer_state.json (deflated 75%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-110/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-110/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-110/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-110/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-110/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-110/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-110/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-110/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-110/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-110/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-110/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-460/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-460/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-460/trainer_state.json (deflated 82%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-460/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-460/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-460/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-460/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-460/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-460/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-460/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-460/scheduler.pt (deflated 55%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-460/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-460/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-460/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-150/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-150/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-150/trainer_state.json (deflated 76%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-150/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-150/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-150/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-150/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-150/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-150/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-150/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-150/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-150/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-150/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-150/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-120/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-120/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-120/trainer_state.json (deflated 75%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-120/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-120/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-120/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-120/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-120/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-120/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-120/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-120/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-120/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-120/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-120/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-130/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-130/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-130/trainer_state.json (deflated 76%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-130/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-130/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-130/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-130/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-130/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-130/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-130/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-130/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-130/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-130/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-130/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-410/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-410/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-410/trainer_state.json (deflated 82%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-410/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-410/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-410/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-410/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-410/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-410/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-410/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-410/scheduler.pt (deflated 55%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-410/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-410/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-410/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-470/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-470/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-470/trainer_state.json (deflated 82%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-470/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-470/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-470/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-470/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-470/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-470/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-470/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-470/scheduler.pt (deflated 55%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-470/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-470/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-470/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-400/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-400/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-400/trainer_state.json (deflated 82%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-400/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-400/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-400/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-400/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-400/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-400/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-400/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-400/scheduler.pt (deflated 55%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-400/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-400/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-400/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-300/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-300/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-300/trainer_state.json (deflated 80%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-300/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-300/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-300/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-300/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-300/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-300/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-300/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-300/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-300/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-300/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-300/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-330/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-330/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-330/trainer_state.json (deflated 81%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-330/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-330/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-330/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-330/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-330/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-330/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-330/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-330/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-330/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-330/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-330/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-200/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-200/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-200/trainer_state.json (deflated 78%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-200/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-200/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-200/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-200/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-200/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-200/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-200/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-200/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-200/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-200/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-200/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-220/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-220/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-220/trainer_state.json (deflated 78%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-220/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-220/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-220/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-220/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-220/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-220/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-220/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-220/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-220/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-220/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-220/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-190/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-190/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-190/trainer_state.json (deflated 78%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-190/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-190/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-190/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-190/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-190/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-190/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-190/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-190/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-190/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-190/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-190/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-430/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-430/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-430/trainer_state.json (deflated 82%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-430/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-430/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-430/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-430/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-430/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-430/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-430/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-430/scheduler.pt (deflated 55%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-430/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-430/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-430/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-20/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-20/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-20/trainer_state.json (deflated 59%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-20/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-20/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-20/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-20/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-20/adapter_model/adapter_model.safetensors (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-20/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-20/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-20/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-20/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-20/optimizer.pt (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-20/adapter_model.safetensors (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-180/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-180/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-180/trainer_state.json (deflated 77%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-180/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-180/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-180/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-180/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-180/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-180/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-180/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-180/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-180/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-180/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-180/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-270/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-270/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-270/trainer_state.json (deflated 79%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-270/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-270/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-270/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-270/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-270/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-270/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-270/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-270/scheduler.pt (deflated 55%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-270/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-270/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-270/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-490/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-490/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-490/trainer_state.json (deflated 83%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-490/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-490/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-490/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-490/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-490/adapter_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-490/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-490/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-490/scheduler.pt (deflated 55%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-490/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-490/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-490/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-40/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-40/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-40/trainer_state.json (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-40/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-40/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-40/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-40/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-40/adapter_model/adapter_model.safetensors (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-40/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-40/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-40/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-40/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-40/optimizer.pt (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-40/adapter_model.safetensors (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-10/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-10/rng_state.pth (deflated 25%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-10/trainer_state.json (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-10/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-10/adapter_model/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-10/adapter_model/adapter_config.json (deflated 54%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-10/adapter_model/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-10/adapter_model/adapter_model.safetensors (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-10/README.md (deflated 66%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-10/training_args.bin (deflated 51%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-10/scheduler.pt (deflated 56%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-10/preprocessor_config.json (deflated 42%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-10/optimizer.pt (deflated 7%)\n",
            "  adding: content/faster-whisper-small-en/checkpoint-10/adapter_model.safetensors (deflated 8%)\n",
            "  adding: content/faster-whisper-small-en/runs/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/runs/Dec02_06-20-05_8808f9cf0358/ (stored 0%)\n",
            "  adding: content/faster-whisper-small-en/runs/Dec02_06-20-05_8808f9cf0358/events.out.tfevents.1733120406.8808f9cf0358.5695.0 (deflated 68%)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Cannot find file: /content/sample_data.zip",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-86a1cd666381>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'zip -r /content/faster-whisper-small-en.zip /content/faster-whisper-small-en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_data.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: /content/sample_data.zip"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('/content/faster-whisper-small-en.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "g5o3dLJyErCb",
        "outputId": "61fe030f-f174-489d-f5d7-fec030573c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_756cbc07-6258-4041-a3e3-49d3d2b69cbf\", \"faster-whisper-small-en.zip\", 2637052385)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-gZa0CzbOPTL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}