{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade transformers accelerate ipython-autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline , AutomaticSpeechRecognitionPipeline , WhisperTokenizer, WhisperTimeStampLogitsProcessor ,WhisperForConditionalGeneration , WhisperProcessor ,AutoModelForSpeechSeq2Seq, AutoModelForCausalLM, AutoProcessor\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2024-11-29 10:59:42 +05:30)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 torch.float16\n",
      "time: 31 ms (started: 2024-11-29 10:59:42 +05:30)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "print(device,torch_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2024-11-29 12:37:40 +05:30)\n"
     ]
    }
   ],
   "source": [
    "#input model\n",
    "model_name = \"openai/whisper-large-v3\"\n",
    "language = \"English\"\n",
    "language_abbr = \"en\"\n",
    "task= 'transcribe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_asr = pipeline(\n",
    "    \"Automatic-Speech-Recognition\",\n",
    "    model = model_name,\n",
    "    chunk_length_s = 30,\n",
    "    device = device \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2024-11-29 11:03:03 +05:30)\n"
     ]
    }
   ],
   "source": [
    "def format_time(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    seconds = seconds % 60\n",
    "    return (f\"{hours:02}:{minutes:02}:{seconds:06.3f}\").replace(\".\", \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def process_audio_and_create_vtt(audiofile_path, audio_type, model, outputfilename=None):\n",
    "    # Transcribe the audio file with timestamps\n",
    "    prediction = model(audiofile_path, return_timestamps=True)\n",
    "    \n",
    "    # Determine output VTT file name\n",
    "    audiofile_name = os.path.splitext(os.path.basename(audiofile_path))[0]  # Extract file name without extension\n",
    "    vtt_file_name = outputfilename if outputfilename else f\"{audiofile_name}.vtt\"\n",
    "\n",
    "    # Write the transcription to the VTT file\n",
    "    with open(vtt_file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"WEBVTT\\n\\n\")\n",
    "        for chunk in prediction[\"chunks\"]:\n",
    "            start_time = format_time(chunk[\"start\"])\n",
    "            end_time = format_time(chunk[\"end\"])\n",
    "            text = chunk[\"text\"]\n",
    "            f.write(f\"{start_time} --> {end_time}\\n{text}\\n\\n\")\n",
    "\n",
    "\n",
    "def process_directory_audio_files(directory_path, model):\n",
    "    \"\"\"\n",
    "    Process all .mp3 files in a directory and create corresponding VTT files.\n",
    "\n",
    "    Parameters:\n",
    "    - directory_path: Path to the directory containing audio files\n",
    "    - model: The transcription model to use\n",
    "    \"\"\"\n",
    "    # Get all .mp3 files in the directory\n",
    "    audio_files = [f for f in os.listdir(directory_path) if f.endswith(\".mp3\")]\n",
    "    \n",
    "    if not audio_files:\n",
    "        print(\"No .mp3 files found in the directory.\")\n",
    "        return\n",
    "\n",
    "    # Process each audio file\n",
    "    for audio_file in audio_files:\n",
    "        audiofile_path = os.path.join(directory_path, audio_file)\n",
    "        output_file = os.path.join(directory_path, f\"{os.path.splitext(audio_file)[0]}.vtt\")\n",
    "        \n",
    "        print(f\"Processing: {audio_file}\")\n",
    "        process_audio_and_create_vtt(audiofile_path, \"mp3\", model, output_file)\n",
    "\n",
    "    print(\"All .mp3 files in the directory have been processed.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
